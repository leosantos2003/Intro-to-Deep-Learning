# Lesson 2

`lesson_2`: deep neural networks; adding hidden layers to the network.

<div style="display: flex; justify-content: center;">
<div class="texto-titulo">

* The Activation Function acts as a "decision filter" at the end of each neuron.
* It introducec nonlinearity. If we didn't use activation functions (or used a linear function), no matter how many layers the network had, it would behave like a simple linear regression.

## Graphic 1:
* The graphic shows the "ReLU" Activation Function. "ReLU" stands for "Rectified Linear Unit".
  * If the input (x) is positive, the output is the input itself.
  * If the input (x) is negative or zero, the output is zero.
* ReLU is extremely popular because it is simple, fast to compute, and solves important problems in training deep networks, all with an incredibly basic rule.

</div>
      <img width="640" height="480" alt="activation_graphic" src="https://github.com/user-attachments/assets/5fce2889-6b84-434d-9c1d-e9bad4583be8" />
</div>
